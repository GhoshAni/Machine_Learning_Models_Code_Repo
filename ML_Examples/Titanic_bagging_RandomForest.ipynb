{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns with null values:\n",
      " PassengerId      0\n",
      "Survived         0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age            177\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          687\n",
      "Embarked         2\n",
      "dtype: int64\n",
      "----------\n",
      "Test columns with null values:\n",
      " PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age             86\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             1\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "----------\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Name        0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n",
      "Survived    0\n",
      "Pclass      0\n",
      "Name        0\n",
      "Sex         0\n",
      "Age         0\n",
      "SibSp       0\n",
      "Parch       0\n",
      "Fare        0\n",
      "Embarked    0\n",
      "dtype: int64\n",
      "PassengerId      0\n",
      "Pclass           0\n",
      "Name             0\n",
      "Sex              0\n",
      "Age              0\n",
      "SibSp            0\n",
      "Parch            0\n",
      "Ticket           0\n",
      "Fare             0\n",
      "Cabin          327\n",
      "Embarked         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\pandas\\core\\indexing.py:179: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr        517\n",
      "Miss      182\n",
      "Mrs       125\n",
      "Master     40\n",
      "Misc       27\n",
      "Name: Title, dtype: int64\n",
      "Bin X Y:  ['Survived', 'Sex_Code', 'Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code'] \n",
      "\n",
      "Dummy X Y:  ['Survived', 'Pclass', 'SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'Title_Master', 'Title_Misc', 'Title_Miss', 'Title_Mr', 'Title_Mrs'] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(     Pclass  SibSp  Parch   Age      Fare  FamilySize  IsAlone  Sex_female  \\\n",
       " 105       3      0      0  28.0    7.8958           1        1           0   \n",
       " 68        3      4      2  17.0    7.9250           7        0           1   \n",
       " 253       3      1      0  30.0   16.1000           2        0           0   \n",
       " 320       3      0      0  22.0    7.2500           1        1           0   \n",
       " 706       2      0      0  45.0   13.5000           1        1           1   \n",
       " 271       3      0      0  25.0    0.0000           1        1           0   \n",
       " 424       3      1      1  18.0   20.2125           3        0           0   \n",
       " 752       3      0      0  33.0    9.5000           1        1           0   \n",
       " 615       2      1      2  24.0   65.0000           4        0           1   \n",
       " 2         3      0      0  26.0    7.9250           1        1           1   \n",
       " 882       3      0      0  22.0   10.5167           1        1           1   \n",
       " 467       1      0      0  56.0   26.5500           1        1           0   \n",
       " 403       3      1      0  28.0   15.8500           2        0           0   \n",
       " 258       1      0      0  35.0  512.3292           1        1           1   \n",
       " 720       2      0      1   6.0   33.0000           2        0           1   \n",
       " 272       2      0      1  41.0   19.5000           2        0           1   \n",
       " 154       3      0      0  28.0    7.3125           1        1           0   \n",
       " 832       3      0      0  28.0    7.2292           1        1           0   \n",
       " 820       1      1      1  52.0   93.5000           3        0           1   \n",
       " 382       3      0      0  32.0    7.9250           1        1           0   \n",
       " 20        2      0      0  35.0   26.0000           1        1           0   \n",
       " 585       1      0      2  18.0   79.6500           3        0           1   \n",
       " 65        3      1      1  28.0   15.2458           3        0           0   \n",
       " 680       3      0      0  28.0    8.1375           1        1           1   \n",
       " 878       3      0      0  28.0    7.8958           1        1           0   \n",
       " 304       3      0      0  28.0    8.0500           1        1           0   \n",
       " 823       3      0      1  27.0   12.4750           2        0           1   \n",
       " 521       3      0      0  22.0    7.8958           1        1           0   \n",
       " 333       3      2      0  16.0   18.0000           3        0           0   \n",
       " 698       1      1      1  49.0  110.8833           3        0           0   \n",
       " ..      ...    ...    ...   ...       ...         ...      ...         ...   \n",
       " 72        2      0      0  21.0   73.5000           1        1           0   \n",
       " 845       3      0      0  42.0    7.5500           1        1           0   \n",
       " 537       1      0      0  30.0  106.4250           1        1           1   \n",
       " 677       3      0      0  18.0    9.8417           1        1           1   \n",
       " 849       1      1      0  28.0   89.1042           2        0           1   \n",
       " 874       2      1      0  28.0   24.0000           2        0           1   \n",
       " 174       1      0      0  56.0   30.6958           1        1           0   \n",
       " 87        3      0      0  28.0    8.0500           1        1           0   \n",
       " 551       2      0      0  27.0   26.0000           1        1           0   \n",
       " 486       1      1      0  35.0   90.0000           2        0           1   \n",
       " 705       2      0      0  39.0   26.0000           1        1           0   \n",
       " 314       2      1      1  43.0   26.2500           3        0           0   \n",
       " 396       3      0      0  31.0    7.8542           1        1           1   \n",
       " 600       2      2      1  24.0   27.0000           4        0           1   \n",
       " 472       2      1      2  33.0   27.7500           4        0           1   \n",
       " 70        2      0      0  32.0   10.5000           1        1           0   \n",
       " 599       1      1      0  49.0   56.9292           2        0           0   \n",
       " 804       3      0      0  27.0    6.9750           1        1           0   \n",
       " 754       2      1      2  48.0   65.0000           4        0           1   \n",
       " 277       2      0      0  28.0    0.0000           1        1           0   \n",
       " 723       2      0      0  50.0   13.0000           1        1           0   \n",
       " 9         2      1      0  14.0   30.0708           2        0           1   \n",
       " 359       3      0      0  28.0    7.8792           1        1           1   \n",
       " 707       1      0      0  42.0   26.2875           1        1           0   \n",
       " 763       1      1      2  36.0  120.0000           4        0           1   \n",
       " 835       1      1      1  39.0   83.1583           3        0           1   \n",
       " 192       3      1      0  19.0    7.8542           2        0           1   \n",
       " 629       3      0      0  28.0    7.7333           1        1           0   \n",
       " 559       3      1      0  36.0   17.4000           2        0           1   \n",
       " 684       2      1      1  60.0   39.0000           3        0           0   \n",
       " \n",
       "      Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Misc  \\\n",
       " 105         1           0           0           1             0           0   \n",
       " 68          0           0           0           1             0           0   \n",
       " 253         1           0           0           1             0           0   \n",
       " 320         1           0           0           1             0           0   \n",
       " 706         0           0           0           1             0           0   \n",
       " 271         1           0           0           1             0           0   \n",
       " 424         1           0           0           1             0           0   \n",
       " 752         1           0           0           1             0           0   \n",
       " 615         0           0           0           1             0           0   \n",
       " 2           0           0           0           1             0           0   \n",
       " 882         0           0           0           1             0           0   \n",
       " 467         1           0           0           1             0           0   \n",
       " 403         1           0           0           1             0           0   \n",
       " 258         0           1           0           0             0           0   \n",
       " 720         0           0           0           1             0           0   \n",
       " 272         0           0           0           1             0           0   \n",
       " 154         1           0           0           1             0           0   \n",
       " 832         1           1           0           0             0           0   \n",
       " 820         0           0           0           1             0           0   \n",
       " 382         1           0           0           1             0           0   \n",
       " 20          1           0           0           1             0           0   \n",
       " 585         0           0           0           1             0           0   \n",
       " 65          1           1           0           0             1           0   \n",
       " 680         0           0           1           0             0           0   \n",
       " 878         1           0           0           1             0           0   \n",
       " 304         1           0           0           1             0           0   \n",
       " 823         0           0           0           1             0           0   \n",
       " 521         1           0           0           1             0           0   \n",
       " 333         1           0           0           1             0           0   \n",
       " 698         1           1           0           0             0           0   \n",
       " ..        ...         ...         ...         ...           ...         ...   \n",
       " 72          1           0           0           1             0           0   \n",
       " 845         1           0           0           1             0           0   \n",
       " 537         0           1           0           0             0           0   \n",
       " 677         0           0           0           1             0           0   \n",
       " 849         0           1           0           0             0           0   \n",
       " 874         0           1           0           0             0           0   \n",
       " 174         1           1           0           0             0           0   \n",
       " 87          1           0           0           1             0           0   \n",
       " 551         1           0           0           1             0           0   \n",
       " 486         0           0           0           1             0           0   \n",
       " 705         1           0           0           1             0           0   \n",
       " 314         1           0           0           1             0           0   \n",
       " 396         0           0           0           1             0           0   \n",
       " 600         0           0           0           1             0           0   \n",
       " 472         0           0           0           1             0           0   \n",
       " 70          1           0           0           1             0           0   \n",
       " 599         1           1           0           0             0           1   \n",
       " 804         1           0           0           1             0           0   \n",
       " 754         0           0           0           1             0           0   \n",
       " 277         1           0           0           1             0           0   \n",
       " 723         1           0           0           1             0           0   \n",
       " 9           0           1           0           0             0           0   \n",
       " 359         0           0           1           0             0           0   \n",
       " 707         1           0           0           1             0           0   \n",
       " 763         0           0           0           1             0           0   \n",
       " 835         0           1           0           0             0           0   \n",
       " 192         0           0           0           1             0           0   \n",
       " 629         1           0           1           0             0           0   \n",
       " 559         0           0           0           1             0           0   \n",
       " 684         1           0           0           1             0           0   \n",
       " \n",
       "      Title_Miss  Title_Mr  Title_Mrs  \n",
       " 105           0         1          0  \n",
       " 68            1         0          0  \n",
       " 253           0         1          0  \n",
       " 320           0         1          0  \n",
       " 706           0         0          1  \n",
       " 271           0         1          0  \n",
       " 424           0         1          0  \n",
       " 752           0         1          0  \n",
       " 615           1         0          0  \n",
       " 2             1         0          0  \n",
       " 882           1         0          0  \n",
       " 467           0         1          0  \n",
       " 403           0         1          0  \n",
       " 258           1         0          0  \n",
       " 720           1         0          0  \n",
       " 272           0         0          1  \n",
       " 154           0         1          0  \n",
       " 832           0         1          0  \n",
       " 820           0         0          1  \n",
       " 382           0         1          0  \n",
       " 20            0         1          0  \n",
       " 585           1         0          0  \n",
       " 65            0         0          0  \n",
       " 680           1         0          0  \n",
       " 878           0         1          0  \n",
       " 304           0         1          0  \n",
       " 823           0         0          1  \n",
       " 521           0         1          0  \n",
       " 333           0         1          0  \n",
       " 698           0         1          0  \n",
       " ..          ...       ...        ...  \n",
       " 72            0         1          0  \n",
       " 845           0         1          0  \n",
       " 537           1         0          0  \n",
       " 677           1         0          0  \n",
       " 849           0         0          1  \n",
       " 874           0         0          1  \n",
       " 174           0         1          0  \n",
       " 87            0         1          0  \n",
       " 551           0         1          0  \n",
       " 486           0         0          1  \n",
       " 705           0         1          0  \n",
       " 314           0         1          0  \n",
       " 396           1         0          0  \n",
       " 600           0         0          1  \n",
       " 472           0         0          1  \n",
       " 70            0         1          0  \n",
       " 599           0         0          0  \n",
       " 804           0         1          0  \n",
       " 754           0         0          1  \n",
       " 277           0         1          0  \n",
       " 723           0         1          0  \n",
       " 9             0         0          1  \n",
       " 359           1         0          0  \n",
       " 707           0         1          0  \n",
       " 763           0         0          1  \n",
       " 835           1         0          0  \n",
       " 192           1         0          0  \n",
       " 629           0         1          0  \n",
       " 559           0         0          1  \n",
       " 684           0         1          0  \n",
       " \n",
       " [668 rows x 17 columns],\n",
       "      Pclass  SibSp  Parch    Age      Fare  FamilySize  IsAlone  Sex_female  \\\n",
       " 495       3      0      0  28.00   14.4583           1        1           0   \n",
       " 648       3      0      0  28.00    7.5500           1        1           0   \n",
       " 278       3      4      1   7.00   29.1250           6        0           0   \n",
       " 31        1      1      0  28.00  146.5208           2        0           1   \n",
       " 255       3      0      2  29.00   15.2458           3        0           1   \n",
       " 298       1      0      0  28.00   30.5000           1        1           0   \n",
       " 609       1      0      0  40.00  153.4625           1        1           1   \n",
       " 318       1      0      2  31.00  164.8667           3        0           1   \n",
       " 484       1      1      0  25.00   91.0792           2        0           0   \n",
       " 367       3      0      0  28.00    7.2292           1        1           1   \n",
       " 704       3      1      0  26.00    7.8542           2        0           0   \n",
       " 346       2      0      0  40.00   13.0000           1        1           1   \n",
       " 196       3      0      0  28.00    7.7500           1        1           0   \n",
       " 535       2      0      2   7.00   26.2500           3        0           1   \n",
       " 310       1      0      0  24.00   83.1583           1        1           1   \n",
       " 14        3      0      0  14.00    7.8542           1        1           1   \n",
       " 350       3      0      0  23.00    9.2250           1        1           0   \n",
       " 145       2      1      1  19.00   36.7500           3        0           0   \n",
       " 614       3      0      0  35.00    8.0500           1        1           0   \n",
       " 803       3      0      1   0.42    8.5167           2        0           0   \n",
       " 144       2      0      0  18.00   11.5000           1        1           0   \n",
       " 708       1      0      0  22.00  151.5500           1        1           1   \n",
       " 778       3      0      0  28.00    7.7375           1        1           0   \n",
       " 270       1      0      0  28.00   31.0000           1        1           0   \n",
       " 474       3      0      0  22.00    9.8375           1        1           1   \n",
       " 319       1      1      1  40.00  134.5000           3        0           1   \n",
       " 519       3      0      0  32.00    7.8958           1        1           0   \n",
       " 141       3      0      0  22.00    7.7500           1        1           1   \n",
       " 880       2      0      1  25.00   26.0000           2        0           1   \n",
       " 642       3      3      2   2.00   27.9000           6        0           1   \n",
       " ..      ...    ...    ...    ...       ...         ...      ...         ...   \n",
       " 727       3      0      0  28.00    7.7375           1        1           1   \n",
       " 428       3      0      0  28.00    7.7500           1        1           0   \n",
       " 54        1      0      1  65.00   61.9792           2        0           0   \n",
       " 45        3      0      0  28.00    8.0500           1        1           0   \n",
       " 172       3      1      1   1.00   11.1333           3        0           1   \n",
       " 447       1      0      0  34.00   26.5500           1        1           0   \n",
       " 597       3      0      0  49.00    0.0000           1        1           0   \n",
       " 204       3      0      0  18.00    8.0500           1        1           0   \n",
       " 240       3      1      0  28.00   14.4542           2        0           1   \n",
       " 672       2      0      0  70.00   10.5000           1        1           0   \n",
       " 39        3      1      0  14.00   11.2417           2        0           1   \n",
       " 283       3      0      0  19.00    8.0500           1        1           0   \n",
       " 308       2      1      0  30.00   24.0000           2        0           0   \n",
       " 18        3      1      0  31.00   18.0000           2        0           1   \n",
       " 769       3      0      0  32.00    8.3625           1        1           0   \n",
       " 156       3      0      0  16.00    7.7333           1        1           1   \n",
       " 482       3      0      0  50.00    8.0500           1        1           0   \n",
       " 210       3      0      0  24.00    7.0500           1        1           0   \n",
       " 647       1      0      0  56.00   35.5000           1        1           0   \n",
       " 50        3      4      1   7.00   39.6875           6        0           0   \n",
       " 852       3      1      1   9.00   15.2458           3        0           1   \n",
       " 239       2      0      0  33.00   12.2750           1        1           0   \n",
       " 646       3      0      0  19.00    7.8958           1        1           0   \n",
       " 122       2      1      0  32.50   30.0708           2        0           0   \n",
       " 788       3      1      2   1.00   20.5750           4        0           0   \n",
       " 167       3      1      4  45.00   27.9000           6        0           1   \n",
       " 306       1      0      0  28.00  110.8833           1        1           1   \n",
       " 379       3      0      0  19.00    7.7750           1        1           0   \n",
       " 742       1      2      2  21.00  262.3750           5        0           1   \n",
       " 10        3      1      1   4.00   16.7000           3        0           1   \n",
       " \n",
       "      Sex_male  Embarked_C  Embarked_Q  Embarked_S  Title_Master  Title_Misc  \\\n",
       " 495         1           1           0           0             0           0   \n",
       " 648         1           0           0           1             0           0   \n",
       " 278         1           0           1           0             1           0   \n",
       " 31          0           1           0           0             0           0   \n",
       " 255         0           1           0           0             0           0   \n",
       " 298         1           0           0           1             0           0   \n",
       " 609         0           0           0           1             0           0   \n",
       " 318         0           0           0           1             0           0   \n",
       " 484         1           1           0           0             0           0   \n",
       " 367         0           1           0           0             0           0   \n",
       " 704         1           0           0           1             0           0   \n",
       " 346         0           0           0           1             0           0   \n",
       " 196         1           0           1           0             0           0   \n",
       " 535         0           0           0           1             0           0   \n",
       " 310         0           1           0           0             0           0   \n",
       " 14          0           0           0           1             0           0   \n",
       " 350         1           0           0           1             0           0   \n",
       " 145         1           0           0           1             0           0   \n",
       " 614         1           0           0           1             0           0   \n",
       " 803         1           1           0           0             1           0   \n",
       " 144         1           0           0           1             0           0   \n",
       " 708         0           0           0           1             0           0   \n",
       " 778         1           0           1           0             0           0   \n",
       " 270         1           0           0           1             0           0   \n",
       " 474         0           0           0           1             0           0   \n",
       " 319         0           1           0           0             0           0   \n",
       " 519         1           0           0           1             0           0   \n",
       " 141         0           0           0           1             0           0   \n",
       " 880         0           0           0           1             0           0   \n",
       " 642         0           0           0           1             0           0   \n",
       " ..        ...         ...         ...         ...           ...         ...   \n",
       " 727         0           0           1           0             0           0   \n",
       " 428         1           0           1           0             0           0   \n",
       " 54          1           1           0           0             0           0   \n",
       " 45          1           0           0           1             0           0   \n",
       " 172         0           0           0           1             0           0   \n",
       " 447         1           0           0           1             0           0   \n",
       " 597         1           0           0           1             0           0   \n",
       " 204         1           0           0           1             0           0   \n",
       " 240         0           1           0           0             0           0   \n",
       " 672         1           0           0           1             0           0   \n",
       " 39          0           1           0           0             0           0   \n",
       " 283         1           0           0           1             0           0   \n",
       " 308         1           1           0           0             0           0   \n",
       " 18          0           0           0           1             0           0   \n",
       " 769         1           0           0           1             0           0   \n",
       " 156         0           0           1           0             0           0   \n",
       " 482         1           0           0           1             0           0   \n",
       " 210         1           0           0           1             0           0   \n",
       " 647         1           1           0           0             0           1   \n",
       " 50          1           0           0           1             1           0   \n",
       " 852         0           1           0           0             0           0   \n",
       " 239         1           0           0           1             0           0   \n",
       " 646         1           0           0           1             0           0   \n",
       " 122         1           1           0           0             0           0   \n",
       " 788         1           0           0           1             1           0   \n",
       " 167         0           0           0           1             0           0   \n",
       " 306         0           1           0           0             0           0   \n",
       " 379         1           0           0           1             0           0   \n",
       " 742         0           1           0           0             0           0   \n",
       " 10          0           0           0           1             0           0   \n",
       " \n",
       "      Title_Miss  Title_Mr  Title_Mrs  \n",
       " 495           0         1          0  \n",
       " 648           0         1          0  \n",
       " 278           0         0          0  \n",
       " 31            0         0          1  \n",
       " 255           0         0          1  \n",
       " 298           0         1          0  \n",
       " 609           1         0          0  \n",
       " 318           1         0          0  \n",
       " 484           0         1          0  \n",
       " 367           0         0          1  \n",
       " 704           0         1          0  \n",
       " 346           1         0          0  \n",
       " 196           0         1          0  \n",
       " 535           1         0          0  \n",
       " 310           1         0          0  \n",
       " 14            1         0          0  \n",
       " 350           0         1          0  \n",
       " 145           0         1          0  \n",
       " 614           0         1          0  \n",
       " 803           0         0          0  \n",
       " 144           0         1          0  \n",
       " 708           1         0          0  \n",
       " 778           0         1          0  \n",
       " 270           0         1          0  \n",
       " 474           1         0          0  \n",
       " 319           0         0          1  \n",
       " 519           0         1          0  \n",
       " 141           1         0          0  \n",
       " 880           0         0          1  \n",
       " 642           1         0          0  \n",
       " ..          ...       ...        ...  \n",
       " 727           1         0          0  \n",
       " 428           0         1          0  \n",
       " 54            0         1          0  \n",
       " 45            0         1          0  \n",
       " 172           1         0          0  \n",
       " 447           0         1          0  \n",
       " 597           0         1          0  \n",
       " 204           0         1          0  \n",
       " 240           1         0          0  \n",
       " 672           0         1          0  \n",
       " 39            1         0          0  \n",
       " 283           0         1          0  \n",
       " 308           0         1          0  \n",
       " 18            0         0          1  \n",
       " 769           0         1          0  \n",
       " 156           1         0          0  \n",
       " 482           0         1          0  \n",
       " 210           0         1          0  \n",
       " 647           0         0          0  \n",
       " 50            0         0          0  \n",
       " 852           1         0          0  \n",
       " 239           0         1          0  \n",
       " 646           0         1          0  \n",
       " 122           0         1          0  \n",
       " 788           0         0          0  \n",
       " 167           0         0          1  \n",
       " 306           1         0          0  \n",
       " 379           0         1          0  \n",
       " 742           1         0          0  \n",
       " 10            1         0          0  \n",
       " \n",
       " [223 rows x 17 columns],\n",
       "      Survived\n",
       " 105         0\n",
       " 68          1\n",
       " 253         0\n",
       " 320         0\n",
       " 706         1\n",
       " 271         1\n",
       " 424         0\n",
       " 752         0\n",
       " 615         1\n",
       " 2           1\n",
       " 882         0\n",
       " 467         0\n",
       " 403         0\n",
       " 258         1\n",
       " 720         1\n",
       " 272         1\n",
       " 154         0\n",
       " 832         0\n",
       " 820         1\n",
       " 382         0\n",
       " 20          0\n",
       " 585         1\n",
       " 65          1\n",
       " 680         0\n",
       " 878         0\n",
       " 304         0\n",
       " 823         1\n",
       " 521         0\n",
       " 333         0\n",
       " 698         0\n",
       " ..        ...\n",
       " 72          0\n",
       " 845         0\n",
       " 537         1\n",
       " 677         1\n",
       " 849         1\n",
       " 874         1\n",
       " 174         0\n",
       " 87          0\n",
       " 551         0\n",
       " 486         1\n",
       " 705         0\n",
       " 314         0\n",
       " 396         0\n",
       " 600         1\n",
       " 472         1\n",
       " 70          0\n",
       " 599         1\n",
       " 804         1\n",
       " 754         1\n",
       " 277         0\n",
       " 723         0\n",
       " 9           1\n",
       " 359         1\n",
       " 707         1\n",
       " 763         1\n",
       " 835         1\n",
       " 192         1\n",
       " 629         0\n",
       " 559         1\n",
       " 684         0\n",
       " \n",
       " [668 rows x 1 columns],\n",
       "      Survived\n",
       " 495         0\n",
       " 648         0\n",
       " 278         0\n",
       " 31          1\n",
       " 255         1\n",
       " 298         1\n",
       " 609         1\n",
       " 318         1\n",
       " 484         1\n",
       " 367         1\n",
       " 704         0\n",
       " 346         1\n",
       " 196         0\n",
       " 535         1\n",
       " 310         1\n",
       " 14          0\n",
       " 350         0\n",
       " 145         0\n",
       " 614         0\n",
       " 803         1\n",
       " 144         0\n",
       " 708         1\n",
       " 778         0\n",
       " 270         0\n",
       " 474         0\n",
       " 319         1\n",
       " 519         0\n",
       " 141         1\n",
       " 880         1\n",
       " 642         0\n",
       " ..        ...\n",
       " 727         1\n",
       " 428         0\n",
       " 54          0\n",
       " 45          0\n",
       " 172         1\n",
       " 447         1\n",
       " 597         0\n",
       " 204         1\n",
       " 240         0\n",
       " 672         0\n",
       " 39          1\n",
       " 283         1\n",
       " 308         0\n",
       " 18          0\n",
       " 769         0\n",
       " 156         1\n",
       " 482         0\n",
       " 210         0\n",
       " 647         1\n",
       " 50          0\n",
       " 852         0\n",
       " 239         0\n",
       " 646         0\n",
       " 122         0\n",
       " 788         1\n",
       " 167         0\n",
       " 306         1\n",
       " 379         0\n",
       " 742         1\n",
       " 10          1\n",
       " \n",
       " [223 rows x 1 columns])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Random Forest\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "from datetime import time\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "import random\n",
    "import time\n",
    "import sklearn\n",
    "import matplotlib as mpl\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "## common model helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "path = 'H:/data/titanic'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "data_raw = pd.read_csv('train.csv', sep = ',')\n",
    "data_val = pd.read_csv('test.csv', sep = ',')\n",
    "data1 = data_raw.copy(deep = True)\n",
    "data_cleaner = [data1, data_val]\n",
    "print('Train columns with null values:\\n', data1.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "\n",
    "print('Test columns with null values:\\n',data_val.isnull().sum())\n",
    "print(\"-\"*10)\n",
    "data_cleaner = [data1, data_val]\n",
    "## treat missing values\n",
    "for dataset in data_cleaner:\n",
    "    \n",
    "    dataset[\"Age\"].fillna(dataset[\"Age\"].median(), inplace = True)\n",
    "    \n",
    "    dataset[\"Embarked\"].fillna(dataset[\"Embarked\"].mode()[0], inplace = True)\n",
    "    \n",
    "    dataset[\"Fare\"].fillna(dataset[\"Fare\"].median(), inplace = True)\n",
    "\n",
    "drop_column = ['PassengerId','Ticket','Cabin']\n",
    "\n",
    "data1.drop (drop_column, axis =1, inplace = True)\n",
    "    \n",
    "print(data1.isnull().sum())\n",
    "print(data_val.isnull().sum())\n",
    "\n",
    "for i in data_cleaner:\n",
    "    \n",
    "    i[\"Age\"].fillna(i[\"Age\"].median(), inplace = True)\n",
    "    \n",
    "    i[\"Embarked\"].fillna(i[\"Embarked\"].mode()[0], inplace = True)\n",
    "    \n",
    "    i[\"Fare\"].fillna(i[\"Fare\"].median(), inplace = True)\n",
    "\n",
    "\n",
    "    \n",
    "print(data1.isnull().sum())\n",
    "print(data_val.isnull().sum())\n",
    "\n",
    "for dataset in data_cleaner:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "    dataset['IsAlone'] = 1\n",
    "    dataset['IsAlone'].loc[dataset['FamilySize'] >1 ] = 0\n",
    "    dataset['Title'] = dataset['Name'].str.split(\", \", expand = True)[1].str.split(\".\", expand = True)[0]\n",
    "    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n",
    "    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n",
    "    \n",
    "    stat_min = 10\n",
    "title_names = (data1['Title'].value_counts()<stat_min)\n",
    "data1['Title'] = data1['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n",
    "print(data1['Title'].value_counts())\n",
    "\n",
    "## convert objects to category using label encoder\n",
    "label = LabelEncoder()\n",
    "for dataset in data_cleaner:\n",
    "    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n",
    "    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n",
    "    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n",
    "    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n",
    "    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n",
    "## define y variable\n",
    "Target  = ['Survived']\n",
    "#define x variables for original features aka feature selection\n",
    "data1_x = ['Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare', 'FamilySize', 'IsAlone'] #pretty name/values for charts\n",
    "data1_x_calc = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code','SibSp', 'Parch', 'Age', 'Fare'] #coded for algorithm calculation\n",
    "data1_xy =  Target + data1_x\n",
    "\n",
    "#define x variables for original w/bin features to remove continuous variables\n",
    "data1_x_bin = ['Sex_Code','Pclass', 'Embarked_Code', 'Title_Code', 'FamilySize', 'AgeBin_Code', 'FareBin_Code']\n",
    "data1_xy_bin = Target + data1_x_bin\n",
    "print('Bin X Y: ', data1_xy_bin, '\\n')\n",
    "\n",
    "#define x and y variables for dummy features original\n",
    "data1_dummy = pd.get_dummies(data1[data1_x])\n",
    "data1_x_dummy = data1_dummy.columns.tolist()\n",
    "data1_xy_dummy = Target + data1_x_dummy\n",
    "print('Dummy X Y: ', data1_xy_dummy, '\\n')\n",
    "\n",
    "## train test split\n",
    "train1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x_calc], data1[Target], random_state = 0)\n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin = model_selection.train_test_split(data1[data1_x_bin], data1[Target] , random_state = 0)\n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy = model_selection.train_test_split(data1_dummy[data1_x_dummy], data1[Target], random_state = 0)\n",
    "\n",
    "## data sets\n",
    "train1_x, test1_x, train1_y, test1_y \n",
    "train1_x_bin, test1_x_bin, train1_y_bin, test1_y_bin \n",
    "train1_x_dummy, test1_x_dummy, train1_y_dummy, test1_y_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(668, 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
     ]
    }
   ],
   "source": [
    "## RandomForest Hyperparameter tuning using randomized searchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "## no of trees in Random forest\n",
    "n_estimators = [int(x) for x in np.linspace( start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "## no. of features to consider for each split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "## Maximym number of levels in a tree\n",
    "max_depth = [int(x) for x in np.linspace(start = 10, stop = 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "## minimum samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "## minimum samples required at leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "## method of selecting for se;ecting sample training for each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "## create the random grid\n",
    "\n",
    "random_grid = {'n_estimators' : n_estimators,\n",
    "               'max_features' : max_features,\n",
    "               'max_depth'    :   max_depth,\n",
    "               'min_samples_split' : min_samples_split,\n",
    "               'min_samples_leaf' : min_samples_leaf,\n",
    "               'bootstrap' : bootstrap }\n",
    "\n",
    "\n",
    "pprint(random_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed:  4.2min finished\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise-deprecating',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params=None, iid='warn', n_iter=100, n_jobs=-1,\n",
       "          param_distributions={'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "          return_train_score='warn', scoring=None, verbose=2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## use Random grid to search for best hyperparameters\n",
    "\n",
    "## first create the base model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "## random search of parameters using 5 fold cross validation\n",
    "## search across 100 different combinations using all cores\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter= 100, cv = 5, verbose=2, random_state= 42,\n",
    "                              n_jobs = -1)\n",
    "\n",
    "## fit the random search model\n",
    "rf_random.fit(train1_x, train1_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'max_depth': 90,\n",
       " 'max_features': 'sqrt',\n",
       " 'min_samples_leaf': 4,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 800}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.85%.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.93      0.88       139\n",
      "           1       0.86      0.71      0.78        84\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       223\n",
      "   macro avg       0.85      0.82      0.83       223\n",
      "weighted avg       0.85      0.85      0.84       223\n",
      "\n",
      "auc is 0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:17: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "## to determine if random search yeilded a better model compare the base model with best random search model\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, roc_auc_score, auc\n",
    "from sklearn.metrics import confusion_matrix, auc, roc_curve, precision_recall_curve, log_loss, f1_score, average_precision_score\n",
    "\n",
    "### base model for RandomForest\n",
    "def evaluate(model, test_features, test_labels) :\n",
    "    predictions = model.predict(test_features)\n",
    "    accuracy = accuracy_score(test_labels , predictions)\n",
    "    class_report = classification_report(test_labels, predictions)\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    print(class_report)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "base_model = RandomForestClassifier(n_estimators= 10, random_state=42)\n",
    "base_model.fit(train1_x, train1_y)\n",
    "base_accuracy = evaluate(base_model, test1_x, test1_y)\n",
    "\n",
    "y_pred_prob = base_model.predict_proba(test1_x)[:,1]\n",
    "auc = roc_auc_score(test1_y, y_pred_prob )\n",
    "print('auc is %3.2f'% auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.84%.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88       139\n",
      "           1       0.83      0.74      0.78        84\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       223\n",
      "   macro avg       0.84      0.82      0.83       223\n",
      "weighted avg       0.84      0.84      0.84       223\n",
      "\n",
      "auc is 0.90\n"
     ]
    }
   ],
   "source": [
    "## try  with random search params\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "best_random.fit(train1_x, train1_y)\n",
    "best_accuracy = evaluate(best_random, test1_x, test1_y)\n",
    "\n",
    "y_pred_prob1 = best_random.predict_proba(test1_x)[:,1]\n",
    "auc_random = roc_auc_score(test1_y, y_pred_prob1 )\n",
    "print('auc is %3.2f'% auc_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 288 candidates, totalling 1440 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:    3.0s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done 632 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 997 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1440 out of 1440 | elapsed:  3.7min finished\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\model_selection\\_search.py:740: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'bootstrap': [True], 'max_depth': [80, 90, 100, 110], 'max_features': [2, 3], 'min_samples_leaf': [3, 4, 5], 'min_samples_split': [8, 10, 12], 'n_estimators': [100, 200, 300, 1000]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## random seacrch allowed to narrow down the rang for each hyper paraeter\n",
    "## now let us try GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "## create param_ grid\n",
    "param_grid =  {'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]}\n",
    "\n",
    "## create a  base model\n",
    "rf = RandomForestClassifier()\n",
    "grid_search = GridSearchCV(estimator= rf, param_grid=param_grid, cv = 5, n_jobs= -1, verbose=2)\n",
    "grid_search.fit(train1_x, train1_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.84%.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.90      0.87       139\n",
      "           1       0.82      0.74      0.78        84\n",
      "\n",
      "   micro avg       0.84      0.84      0.84       223\n",
      "   macro avg       0.83      0.82      0.82       223\n",
      "weighted avg       0.84      0.84      0.84       223\n",
      "\n",
      "auc is 0.90\n",
      "Improvement of accuracy is -1.06%.\n",
      "Improvement of auc is 0.12%.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "grid_search.best_params_\n",
    "best_grid = grid_search.best_estimator_\n",
    "best_grid.fit(train1_x, train1_y)\n",
    "grid_accuracy = evaluate(best_grid, test1_x, test1_y)\n",
    "y_pred_prob2 = best_grid.predict_proba(test1_x)[:,1]\n",
    "auc_grid = roc_auc_score(test1_y, y_pred_prob2 )\n",
    "print('auc is %3.2f'% auc_grid)\n",
    "\n",
    "## how much improement from  random search to grid scearch\n",
    "print('Improvement of accuracy is {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) / base_accuracy))\n",
    "print('Improvement of auc is {:0.2f}%.'.format( 100 * (auc_grid - auc_random) / auc_random))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##what is called the Bayes error rate,\n",
    "##which is the absolute minimum possible error in a problem. Bayes error, \n",
    "## also called reproducible error, is a combination of latent variables, \n",
    "##the factors affecting a problem which we cannot measure, and inherent noise in any physical process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'H:/data/titanic'\n",
    "os.chdir(path)\n",
    "os.getcwd()\n",
    "train  = pd.read_csv('train.csv', sep = ',')\n",
    "test = pd.read_csv('test.csv', sep = ',')\n",
    "#Checking for missing data\n",
    "NAs = pd.concat([train.isnull().sum()], axis=1, keys=['Train'])\n",
    "NAs[NAs.sum(axis=1) > 0]\n",
    "train.pop('Cabin')\n",
    "train.pop('Name')\n",
    "train.pop('Ticket')\n",
    "train.shape\n",
    "# Filling missing Age values with mean\n",
    "train['Age'] = train['Age'].fillna(train['Age'].mean())\n",
    "# Filling missing Embarked values with most common value\n",
    "train['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode()[0])\n",
    "train['Pclass'] = train['Pclass'].apply(str)\n",
    "# Getting Dummies from all other categorical vars\n",
    "for col in train.dtypes[train.dtypes == 'object'].index:\n",
    "    for_dummy = train.pop(col)\n",
    "    train = pd.concat([train, pd.get_dummies(for_dummy, prefix=col)], axis=1)\n",
    "train.head()\n",
    "labels = train.pop('Survived')\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train, labels, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73436189837330201"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## baseline performance\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = rf.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAECCAYAAADzStBRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3XlcVPX+P/DXLGyyiUspqaQm9+JK\ng9ebCu5ImmlKJYajN3GrX2r3muLa1xRxuZm5pLmgJYqiaDfF+9DrcnUUzZLEQotbpgZpaALJDJvM\nfH5/HBkckUCdBTiv5+Mxj+Gcw8y8D9nnNZ/POedzFEIIASIiki2lowsgIiLHYhAQEckcg4CISOYY\nBEREMscgICKSObWjC3gYRUVFSE9PR+PGjaFSqRxdDhFRrWA0GnHz5k20b98erq6uFbbXqiBIT09H\nZGSko8sgIqqVtm3bhs6dO1dYX6uCoHHjxgCknWnSpImDqyEiqh1+/fVXREZGmtvQ+9WqICgbDmrS\npAmaNWvm4GqIiGqXyobUebCYiEjmGARERDLHICAikjkGARGRzNk0CM6fPw+tVlth/dGjRxEeHo7h\nw4dj586dAKRrBCZNmoTXXnsN48aNQ05Oji1LIyKiu2x21tCGDRuwd+9euLm5Way/c+cOFi1ahKSk\nJLi5uWHEiBHo3bs3kpOT4e/vj0mTJmH//v1Ys2YN5syZY6vyZEEI6UFEdYPSRl/dbRYELVq0wKpV\nqzB9+nSL9ZcuXUKLFi3g7e0NAAgKCsLZs2eRmpqKsWPHAgB69OiBNWvW2Kq0GslkAgoKAL1eeuTn\nl/9cneUHrTMYHL1XRGQtLi7Af/8LdO1q/fe2WRCEhYUhKyurwnq9Xg9PT0/zsru7O/R6vcV6d3d3\n5Ofn26o0h8vOBr76CvjyS+mRmgrculX9b+9qNeDpCXh4lD88PQE/P8tld3eAM3EQ1Q2urkBAgG3e\n2+4XlHl4eMBwz1dVg8EAT09Pi/UGgwFeXl72Ls0m9HqpoS9r9L/8Evj5Z2mbUgl06AAMGQI89VTF\nhr2yZWdnQKFw7H4RUd1h9yBo3bo1rl69iry8PNSrVw9nz55FVFQUrl27huPHj6Njx47Q6XQICgqy\nd2mP7c4dID3dstG/eFEa9gGAli2lbt2UKUCXLsCzz0rf2omIHMluQbBv3z4UFBRg+PDhmDFjBqKi\noiCEQHh4OJ588kmMGDEC0dHRGDFiBJycnLBs2TJ7lWam1wMrV0pjcQ0aAD4+ls8NGgBlx76FAC5d\nshzi+fproKhI2t6okdTYh4dLz3/5C1DJNB9ERA6lqE03r8/KykLfvn1x5MgRm8w1tH49MGHCH/9O\nWUgUFQG5udI6NzcgKEhq8MseTz/N4Rsiqhmqajtr1aRztqbTAU2aAN9/LzXyublATo70uP9npRLo\n3Flq9Nu1kw7gEhHVRmy+7hICOH4c6NkT8PaWHk8/7eiqiIhsj1NM3HXlCpCVBfTo4ehKiIjsi0Fw\n1/Hj0nPPno6tg4jI3hgEd+l0QMOGtrtgg4iopmIQ3HX8uDQsZKu5PIiIaio2e5CODfz0E48PEJE8\nMQggDQsBPD5ARPLEIIA0LOTtDXTs6OhKiIjsj0EAqUcQHMyZOolInmQfBNnZ0pXEHBYiIrmSfRCc\nOCE980AxEcmV7IPg+HFpKmiNxtGVEBE5huyDQKcDunUDnJwcXQkRkWPIOghycoBvv+XxASKSN1kH\nwYkT0qyjPD5ARHIm6yDQ6aQbzXTp4uhKiIgcR9ZBcPw48NxzUhgQEcmVbIPg9m3g3DkeHyAikm0Q\nXLwImEzSTeWJiORMtkHw66/S81NPObYOIiJHk30QPPmkY+sgInI0WQeBQgE0buzoSoiIHEvWQdCo\nEa8oJiKSdRA0aeLoKoiIHI9BQEQkc7INguxsBgERESDTIBCCPQIiojKyDILbt4GiIgYBEREg0yDg\nNQREROVkHQTsERARMQiIiGSPQUBEJHOyDILsbOmKYh8fR1dCROR4sgyCX3+VDhQrZbn3RESW1LZ6\nY5PJhHnz5iEjIwPOzs6IiYmBn5+fefv69euxf/9+eHh4YOzYsejduzfy8vIQFhYGf39/AEC/fv0w\nevRoq9fGawiIiMrZLAgOHz6MkpISJCYmIi0tDYsXL8batWsBABkZGUhOTsauXbsAABEREXjuuedw\n8eJFDBo0CHPnzrVVWQCkIPD1telHEBHVGjYbHElNTUVISAgAIDAwEOnp6eZtly5dQpcuXeDi4gIX\nFxf4+fkhIyMD6enpuHDhAkaOHInJkyfjxo0bNqmNPQIionI2CwK9Xg8PDw/zskqlQmlpKQDgT3/6\nE86ePQu9Xo/c3FycO3cOhYWFaNWqFSZPnoytW7eiX79+iImJsXpdRiNw4waDgIiojM2Ghjw8PGAw\nGMzLJpMJarX0ca1bt0ZkZCTGjRsHPz8/dOrUCT4+PujQoQPc3NwAAKGhoVi5cqXV67p1SwoDBgER\nkcRmPQKNRgOdTgcASEtLMx8ABoCcnBzk5uZi+/btmD17Nq5fv442bdpgzpw5OHjwIADg9OnTaNeu\nndXrys6WnhkEREQSm/UIQkNDkZKSgoiICAghEBsbi82bN6NFixbo06cPsrKyEB4eDicnJ0yfPh0q\nlQpTp07FrFmzsH37dri5udlkaIgXkxERWbJZECiVSsyfP99iXevWrc0/378NAJo3b474+HhblQSA\nQUBEdD/ZXVLFICAisiTLIKhXD7jnhCYiIlmTXRDcvAk0buzoKoiIag7ZBUFBAeDu7ugqiIhqDtkF\nQWGhNDREREQS2QVBQQFw95o1IiKCDIOgsJBBQER0L1kGAYeGiIjKyS4IODRERGRJdkHAoSEiIkuy\nC4KCAg4NERHdS3ZBwB4BEZElWQWByQQUFbFHQER0L1kFQVGR9MweARFROVkFQWGh9MwgICIqJ8sg\n4NAQEVE5WQVBQYH0zB4BEVE5WQUBh4aIiCqSVRCU9Qg4NEREVE5WQcAeARFRRQwCIiKZk1UQcGiI\niKgiWQUBewRERBVVKwgOHDiA5cuXo7CwEMnJybauyWZ4HQERUUVVBsH69euxfft2HDhwAEVFRVi9\nejU++ugje9RmdbyOgIiooiqDYP/+/diwYQPc3Nzg4+ODnTt31tpeAYeGiIgqqjII1Go1nJ2dzcte\nXl5Qq9U2LcpWCgoApRK4Z3eIiGSvyha9adOmOHbsGBQKBUpKShAXF4ennnrKHrVZXdm9CBQKR1dC\nRFRzVBkEc+fOxfTp05GRkYHAwEB06tQJy5Yts0dtVseb0hARVVRlEHz77bf49NNPUVhYCKPRCA8P\nD3vUZRO8TSURUUVVHiNYvnw5AMDNza1WhwDAHgER0YNU2SPw9/fH2rVr0blzZ9S75+t0u3btbFqY\nLTAIiIgqqjIIzp8/j/Pnz2PXrl3mdQqFAkeOHLFpYdb2v/8B+/YBXbs6uhIiopqlyiA4evSoPeqw\nuS++kJ5//tmxdRAR1TRVBkFBQQGWLl0KnU6H0tJSdO/eHbNnz651xwv++lfp+ZdfHFsHEVFNU+XB\n4kWLFqGkpAQfffQR1qxZA4VCgQULFtijNqvy95eenZwcWwcRUU1TrWMEe/fuNS/HxMTghRdeqPKN\nTSYT5s2bh4yMDDg7OyMmJgZ+fn7m7evXr8f+/fvh4eGBsWPHonfv3sjJycE777yDoqIiPPHEE1i0\naBHcrHR0V6EAUlKARo2s8nZERHVGlT0Co9EIk8lkXjaZTFCpVFW+8eHDh1FSUoLExERMnToVixcv\nNm/LyMhAcnIydu7ciU2bNmHlypUoLCzEmjVrMGjQICQkJKBt27ZITEx8xN16sG7dynsGREQkqTII\nunbtirfffhunT5/G6dOn8Y9//ANdunSp8o1TU1MREhICAAgMDER6erp526VLl9ClSxe4uLjAxcUF\nfn5+yMjIsHhNjx49cOrUqUfdLyIiqqYqg2DGjBlo06YNPvjgA/zzn/9Eq1atEB0dXeUb6/V6iwPK\nKpUKpaWlAIA//elPOHv2LPR6PXJzc3Hu3DkUFhZCr9fD09MTAODu7o78/PxH3S8iIqqmak0j6ufn\nh127duHmzZvYv38/nKpxxNXDwwMGg8G8bDKZzLOWtm7dGpGRkRg3bhz8/PzQqVMn+Pj4mF/j6uoK\ng8EALy+vR9wtIiKqrip7BPPmzcOxY8ekX1YqkZqaitjY2CrfWKPRQKfTAQDS0tLgf8/gfE5ODnJz\nc7F9+3bMnj0b169fR5s2baDRaHD8+HEAgE6nQ1BQ0KPsExERPYQqewRpaWnmG9E0bNgQK1aswJAh\nQ6p849DQUKSkpCAiIgJCCMTGxmLz5s1o0aIF+vTpg6ysLISHh8PJyQnTp0+HSqXCG2+8gejoaOzc\nuRM+Pj61dpZTIqLapMoguHPnDkpKSsw3pykb56+KUqnE/PnzLda1bt3a/PP92wCgUaNGiIuLq9b7\nExGRdVQZBL169UJUVBSGDBkChUKB5ORk9OzZ0x61ERGRHVQZBNOnT8e2bdtw5MgRqNVqhIaGIiIi\nwh61ERGRHVQZBCqVCqNGjcKoUaOQnZ2NzMxMKJVVHmMmIqJaosoWPSEhAVOnTkVOTg6GDRuG2bNn\n8yAuEVEdUmUQJCUlYebMmThw4AD69OmD/fv3IyUlxR61ERGRHVQZBAqFAo0aNcLp06fRtWtXqNVq\ni7mHiIiodqsyCJydnbFhwwZ8+eWX6N69OxISEqw2IygRETlelUGwcOFCXLlyBUuWLIG3tzdSU1Ox\ncOFCe9RGRER2UOVZQ61atbJo+HmgmIiobuF5oEREMscgICKSOQYBEZHM/WEQ7N69G9988415eenS\npfjss89sXhQREdlPpUGQlJSEdevWWdyEJigoCGvXrsW//vUvuxRHRES2V2kQJCQk4JNPPkFAQIB5\nXd++fREXF4ctW7bYpTgiIrK9SoNACAFfX98K65s3bw6j0WjTooiIyH4qDQKj0fjAqSRMJlO1b05D\nREQ1X6VB0KVLF3zyyScV1m/evBkdOnSwZU1ERGRHlV5ZPGXKFIwcORKHDx+GRqOByWRCWloa9Hr9\nAwOCiIhqp0qDwNPTE7t27cL+/ftx4cIFKBQKREZGon///hZnEhERUe32h3MNOTs7Y+jQoRg6dKi9\n6iEiIjurNAi0Wi0UCoV5WaVSoX79+ujZsydeeukluxRHRES2V2kQjBw50mLZZDLh1q1biI+PR25u\nLl5//XWbF0dERLZXaRCEhYU9cP2LL74IrVbLICAiqiMeetI5b29viyEjIiKq3R46CIQQvKCMiKgO\nqXRoKC8v74Hr4uPjERgYaNOiiIjIfioNgueeew4KhQJCCACAQqGAj48PevbsidmzZ9utQCIisq1K\ng+D777+vsK60tBQHDhzA66+/jl27dtm0MCIiso8qb14PAL///jsSExOxbds2FBQUVDi1lIiIaq8/\nDIKffvoJn376Kfbu3YunnnoKRUVFOHr0KDw9Pe1VHxER2VilZw2NHz8eI0eOhJOTE7Zs2YLk5GS4\nu7szBIiI6phKg+DixYto164d2rRpAz8/PwDg9QNERHVQpUFw7NgxDB06FMnJyQgODsbkyZNRXFxs\nz9qIiMgOKg0CtVqNgQMHIj4+Hnv27METTzyB4uJi9O/fH9u3b7dnjfQodDqgSxcgLAxYuFBaLipy\ndFVEVANV68riZ555BnPmzIFOp0NUVBR27txZ5WtMJhPeffddDB8+HFqtFlevXrXYHhcXh2HDhiE8\nPByHDh0CIF21HBISAq1WC61Wi2XLlj3CLsncnTvAnDlAr17Ab78B169Lyz17At7eQI8ewOzZwMGD\nQH6+o6sloppA2MjBgwdFdHS0EEKIc+fOiYkTJ5q3/f7776Jnz56iuLhY5OXliV69egkhhLhy5YqY\nMGFCpe+ZmZkp/P39RWZmpq3Krt1+/FGIv/5VCECIMWOEyM+X1t+6JcTevUJMmyZtV6ul31EqhQgK\nEuLtt4XYs0eIGzccWz8R2URVbWe1riN4FKmpqQgJCQEABAYGIj093bzNzc0Nvr6+KCwsRGFhofkg\n9IULF5CdnQ2tVgtXV1fMnDkTrVq1slWJdYcQwJYtwFtvAWo1sHMn8Mor5dsbNABefFF6AIDBAHzx\nhTRcdOIE8PHHwIcfStsCAoCQEKnnEBICtGhh//0hIruyWRDo9Xp4eHiYl1UqFUpLS6FWSx/ZtGlT\nvPDCCzAajZgwYQIAoHHjxhg/fjwGDBiAs2fPYtq0adi9e7etSqwb8vKAN94AduyQGu/4+Kobb3d3\noG9f6QEAJSVAaqoUDDodkJgIrF8vbfPzKw+GHj0Af3+AZ48R1Sk2CwIPDw8YDAbzsslkMoeATqfD\njRs3cOTIEQBAVFQUNBoN2rdvD5VKBQDo3LkzsrOzIYTgaauVOXkSGDkSyMqSDghHRwN3/34PxdkZ\n6NpVekRHA0Yj8O23Um9BpwMOHQK2bpV+94knpGAoC4eOHR/tM4moxrBZEGg0Gvz3v//FwIEDkZaW\nBn9/f/M2b29vuLq6wtnZGQqFAp6enrh9+zZWr16N+vXrY9y4cfj+++/h6+vLEHiQ0lJg/nyp8W/Z\nEjh1SjpDyFpUKiAwUHpMmiQNPf3wQ/lQkk4HlPXUvLyA7t3Lh5I6dwZcXKxXCxHZnM2CIDQ0FCkp\nKYiIiIAQArGxsdi8eTNatGiBvn374tSpU3j11VehVCqh0WjQvXt3dOjQAdOmTcPx48ehUqmwaNEi\nW5VXe/30ExAZKY3xjx4NrFoF2Ppqb4VCGhLy9wfGjpXWZWaWh8KJE8DMmdJ6V1fgr38tD4auXYF7\nhgiJqOZRCHF3nulaICsrC3379sWRI0fQrFkzR5djf1u3Am++CSiVwLp1wPDhjq6o3G+/SUNVZcHw\n9deAyST1LjSa8mAIDgYaNnR0tUSyUlXbabMeAVnR779LAZCQIDWkW7dKB3FrkkaNgJdekh6AdI3C\n6dPlB6BXrwbKrgtp16784HNICPDUU46rm6imKi2VTuQoLpYeSqV0jM4GGAT3Ky6uWWPcp05JQ0GZ\nmcCCBdIQTG04OOvpCfTvLz0A6armr74qH07auhVYu1ba1qqV5SmrzzzDM5PIfoSQGtx7G917Hw+7\n/lFe86D1JlPFWg8fLj/bz4rkHQQGA3D2LHDmTPnj2jVpWoZx46Tz7p2cHFNbaal0MHj+fOnb/4kT\n0nh7beXqWn620axZ0v6dP18+lLR/P/Dpp9LvNmliGQwdOkjfhqj2Mxod17BWtq2kxLr76OwsfZm8\n/3Hvend36fqe+9f/0Wu8vKQRARuQbxBkZgJ//jNQUCAtt2olNTxNmkgXZIWHS92wv/1NOkDapo39\nartyRTotNCUF0GqlYRUvL/t9vj2o1UBQkPT4+9+lb2Xff18+lKTTAWV3watfX/ofoCwcNBrpfxCq\nnBDSdCPWakCt1UgbjdbbR6Wy6sbTxQXw8Xm4Bvdx1js51crerHyDID1dCoGVK4GICKBx4/JtS5cC\nBw4AGzZI49pLl0pz94wbBwwbJn27tZWEBOkCsbKfR4yw3WfVJAqFdFVzQAAwYYLUkF29anlmUnKy\n9LtublLvqCwYnnsOqFfPcbWbTI79lvug9SUl0t/QWpycqm4M3dyk0LZHg+viUjuGSGsJ+QZBVpb0\nPGSIZQgA0rfVQYOkx7VrwCefABs3SmP1Pj7St/TwcOkAqZeX9PDweLzhi9u3gf/3/6Sx827dgG3b\ngKeffvT3q+0UCmn/n35a+nsDQHZ2+ZlJOp00bCaE9N+rc+fys5I8POz7Lbe01Lr7XZ3G0Nvbfg2u\nkxOH5uo4+QZBZqb0j9vX949/z9dXGtOeMQM4elQKhLVrpZ7E/Tw8pFDw9CwPiOos6/XA5MnSN+B5\n86TZQdXy/U9TqSeflAI4PFxazsuTDqaX9Ro+/BD45z+r/35qdfUaQ09P+zS4zs5STbVwaIFqN/m2\nNpmZQNOm1W9wlUqgXz/p8dtv0hkwt29Lj/z88p/vX87OtlyubIz06aelBq1bN6vtYp1Xvz4wcKD0\nAKShvrQ0aWy8Ot+W+S2XCIDcg6B580d7baNGwIABD/86IYDCQstgyM+XGrDg4Lp3QNje6tVjkBI9\nAnkHQadO9v1MhUJqrOrVk4Y5iIhqAHn2jYWQgkCO01QQEd1HnkGQmysN0Tzq0BARUR0izyDIzJSe\nGQRERAwCIiK5YxAQEcmcfINAreaZO0REkHMQ+PpyrhIiIsg1CLKyOCxERHSXPIPgca4qJiKqY+QX\nBEKwR0BEdA/5BcHNm9LUwQwCIiIAcgwCnjpKRGRBvkHAeYaIiADIMQjK7kzGHgEREQA5BkFmpnRj\nkvtvT0lEJFPyC4Lr16U7k/HuVEREAOQYBHq9dA9aIiICIMcgMBgAd3dHV0FEVGMwCIiIZI5BQEQk\ncwwCIiKZYxAQEckcg4CISOYYBEREMievICgpAUpLGQRERPeQVxDo9dIzg4CIyExtqzc2mUyYN28e\nMjIy4OzsjJiYGPj5+Zm3x8XFYf/+/VAoFJg4cSJCQ0NRVFSEadOm4datW3B3d8eSJUvQoEED6xVl\nMEjPDAIiIjOb9QgOHz6MkpISJCYmYurUqVi8eLF52+3btxEfH48dO3Zg06ZNiI2NBQBs374d/v7+\nSEhIwEsvvYQ1a9ZYtygGARFRBTYLgtTUVISEhAAAAgMDkZ6ebt7m5uYGX19fFBYWorCwEAqFosJr\nevTogdOnT1u3KAYBEVEFNhsa0uv18PDwMC+rVCqUlpZCrZY+smnTpnjhhRdgNBoxYcIE82s8704I\n5+7ujvz8fOsWxSAgIqrAZkHg4eEBQ1nDC+mYQVkI6HQ63LhxA0eOHAEAREVFQaPRWLzGYDDAy8vL\nukUxCIiIKrDZ0JBGo4FOpwMApKWlwd/f37zN29sbrq6ucHZ2houLCzw9PXH79m1oNBocP34cgBQW\nQUFB1i2KQUBElVi8eDG0Wi2ef/559OrVC1qtFpMnT67Wa7/77jusXr36oT5v4sSJmDhxosW6Pn36\noLi42Lx86dIlaLVaANKX6Y8//hivvfYatFottFotMjIyHuozK2OzHkFoaChSUlIQEREBIQRiY2Ox\nefNmtGjRAn379sWpU6fw6quvQqlUQqPRoHv37ggKCkJ0dDRGjBgBJycnLFu2zLpFMQiIqBIzZswA\nAOzZswc//fQT3nnnnWq/NiAgAAEBAdX+/evXr6OgoAB37txBZmYmmlfj1rkbN25Ebm4utm7dCqVS\niW+++QZvvvkmDhw4ACcnp2p/9oPYLAiUSiXmz59vsa5169bmnydPnlwhbd3c3LBy5UpblcQgIKqF\ntmwBNm16tNeOGQOMGvV4n3/mzBm8//77cHJywquvvgpXV1ds27bNvH3FihX44YcfsGPHDixfvhz9\n+/eHRqPB5cuX0bBhQ6xatQoqlcriPZOSktC3b1+4uroiISEB0dHRVdaRmJiIPXv2QHn37oodO3ZE\nUlLSY4cAILcLyhgERPQIiouLzae1X7lyBevXr0d8fDxatmyJkydPWvxuZmYmpkyZgsTEROTk5ODb\nb7+12G4ymZCcnIwhQ4bghRdewL///W8UFRVVWUNRURG8vb0t1vn4+Dz+zsGGPYIaqSwI6tVzbB1E\nVG2jRj3+t/rH1bJlS/PPDRs2RHR0NNzd3fHTTz8hMDDQ4nd9fHzQtGlTANLZkfeO+QPAiRMnYDAY\nMHXqVABSMOzbtw+vvPIKXFxcUFJSAhcXFwBAQUEBXF1dAQBeXl4VzsY8dOgQunbtarHuUcivR+Dq\nCtzXTSMi+iNlwzH5+flYuXIlli9fjpiYGLi4uEAIYfG7ZddFVSYpKQkxMTGIi4tDXFwcPvzwQyQk\nJAAA2rZti4MHD5p/V6fToUOHDgCAoUOHYvXq1ebP+/rrr7Fo0SI4Ozs/9v7Jr0fwmMlJRPLl4eEB\njUaDoUOHol69evDy8sKNGzfQrFmzar3+1q1bOH/+PJYvX25eFxQUhOLiYnz99deYPn065s6di+3b\nt0OtVqN58+Z47733AEin2a9YsQLDhw+HWq2GWq3G2rVrrRIECnF/nNVgWVlZ6Nu3L44cOVLtP7yF\n0aOB48eBK1esXhsRUU1VVdspv6EhHigmIrLAICAikjl5BYFezyAgIrqPvIKAPQIiogoYBEREMiev\nIMjLA6x0JR4RUV0hn+sIhABychgERPRAixcvxoULF3Dz5k0UFRWhefPm8PHxeaj5z7KysvDDDz+g\nd+/eFbZdv34dYWFhWLZsGUJDQwEAp06dwp49e/D++++bf2/JkiX485//jCFDhuDatWtYvHgxcnNz\nUVhYiE6dOmHGjBlWmV/oXvLpEej1QGkpYM17IBNRnTFjxgzEx8dj/PjxGDRoEOLj4x96EszTp08j\nLS3tgdt2796N0aNHW0xY90dKS0vx5ptvYuzYsYiPj0dSUhKEEA893XV1yKdHkJsrPTMIiGq+x5ly\ntDKPMRXp0qVLce7cOZhMJkRFRaF///7YsmUL9u3bB6VSib/85S+YPHkyNm7ciJKSEjz77LPo1auX\n+fVl8wklJiZi7NixuHTpksVszA/y1VdfoXnz5ujYsaN53fTp0ytMaWEN8ukR5ORIzwwCInoIR48e\nRXZ2NrZv345PP/0Uq1atgl6vx549e/Duu+9ix44daNq0KVQqFcaOHYvBgwdbhAAAnDx5Eu3atUP9\n+vURHh5unluoMgqFAtnZ2RWuAnZ1dYWbm5u1d1FGPQIGAVHtUROmHL3rf//7H9LT0813CjMajbh2\n7RqWLFmCTZs24ZdffoFGo/nDb+q7du3CtWvXEBUVhTt37iAjIwN///vfzbON3stgMMDV1RU+Pj7m\nOzaWKZvWumfPnlbdR/n1CHiwmIgeQqtWrdC1a1fEx8fjk08+wfPPP49mzZph165dWLBgAbZu3Yrz\n58/j/PnzUCgUFQLh1q1buHDhAnbt2oW4uDhs2bIFffr0weeff45nnnkG6enp+O233wBI9xxITU1F\n27Zt8eyzz+Ly5ctIT08HIA0vrVy5EqmpqVbfR/n0CHiMgIgeQWhoKL788ku89tprKCgoQFhYGOrV\nq4fWrVsjPDzcfP+BDh06wNk0tPbZAAAJ7klEQVTZGRs2bEBAQAAGDBgAQLr15fPPP2+eyhoAXnnl\nFcydOxeRkZGYNm0axo0bB1dXV9y5cwevv/66eUhoxYoVWLBgAYqLi2EwGKDRaDBp0iSr76N8Zh9N\nSAAmTQJ++UW6JwERkUxw9tEyERHA1asMASKi+8gnCJRK3pSGiOgB5BMERET0QAwCIiKZYxAQEckc\ng4CISOYYBEREMscgICKSuVp1ZbHRaAQA/Prrrw6uhIio9ihrM8va0PvVqiC4efMmACAyMtLBlRAR\n1T43b96En59fhfW1aoqJoqIipKeno3HjxlCpVI4uh4ioVjAajbh58ybat28P1wfMrlCrgoCIiKyP\nB4uJiGSuVh0jeBQmkwnz5s1DRkYGnJ2dERMT88Axstrs/PnzeP/99xEfH4+rV69ixowZUCgUaNOm\nDf7v//4PSqUSq1evxrFjx6BWqzFr1iyL29/VNnfu3MGsWbPwyy+/oKSkBG+88QaeeeaZOr3fRqMR\nc+bMweXLl6FSqbBo0SIIIer0PgPSXP7Dhg3Dpk2boFar6/z+AsBLL70ET09PAECzZs0wfPhwLFy4\nECqVCsHBwXjrrbes366JOu7gwYMiOjpaCCHEuXPnxMSJEx1ckXWtX79eDBo0SLzyyitCCCEmTJgg\nvvjiCyGEEHPnzhX/+c9/RHp6utBqtcJkMolffvlFDBs2zJElP7akpCQRExMjhBAiJydH9OzZs87v\n96FDh8SMGTOEEEJ88cUXYuLEiXV+n0tKSsSbb74p+vfvL3788cc6v79CCFFUVCSGDBlisW7w4MHi\n6tWrwmQyibFjx4r09HSrt2t1fmgoNTUVISEhAIDAwEDz3X7qihYtWmDVqlXm5QsXLqBLly4AgB49\neuDUqVNITU1FcHAwFAoFfH19YTQakVN2x7Za6Pnnn8eUKVPMyyqVqs7vd79+/bBgwQIAwLVr19Co\nUaM6v89LlixBREQEnnjiCQDy+Lf9/fffo7CwEGPGjMGoUaPw1VdfoaSkBC1atIBCoUBwcDBOnz5t\n9XatzgeBXq+Hxz3TT6tUKpSWljqwIusKCwuDWl0+wieEgEKhAAC4u7sjPz+/wt+gbH1t5e7uDg8P\nD+j1ekyePBlvv/22LPZbrVYjOjoaCxYsQFhYWJ3e5z179qBBgwbmxg6Qx79tV1dXREVFIS4uDu+9\n9x5mzpxpcbP6yvb7cdu1Oh8EHh4eMBgM5mWTyWTRcNY1994Oz2AwwMvLq8LfwGAwmMcga6vr169j\n1KhRGDJkCF588UXZ7PeSJUtw8OBBzJ07F8XFxeb1dW2fd+/ejVOnTkGr1eK7775DdHS0xTf9ura/\nZVq2bInBgwdDoVCgZcuW8PT0RF5ennl7Zfv9uO1anQ8CjUYDnU4HAEhLS4O/v7+DK7Kttm3b4syZ\nMwAAnU6Hzp07Q6PR4OTJkzCZTLh27RpMJhMa1OJ7N//2228YM2YMpk2bhpdffhlA3d/vf/3rX1i3\nbh0AwM3NDQqFAu3bt6+z+7xt2zZs3boV8fHxCAgIwJIlS9CjR486u79lkpKSsHjxYgBAdnY2CgsL\nUa9ePfz8888QQuDkyZPm/bZmu1Z3vxrfFRoaipSUFEREREAIgdjYWEeXZFPR0dGYO3cuPvjgA7Rq\n1QphYWFQqVTo3Lkzhg8fDpPJhHfffdfRZT6Wjz/+GLdv38aaNWuwZs0aAMDs2bMRExNTZ/e7f//+\nmDlzJiIjI1FaWopZs2ahdevWdf6/9b3k8G/75ZdfxsyZMzFixAgoFArExsZCqVTinXfegdFoRHBw\nMDp16oQOHTpYtV3jBWVERDJX54eGiIjojzEIiIhkjkFARCRzDAIiIpljEBARyRyDgIhI5hgERA9h\nzJgx5itcx40bhx9//NEq7/vNN9/U+nPgqfaq8xeUEVlTSkqK+ecNGzZY7X1//PFHZGdnW+39iB4G\nLyijOuHMmTNYvnw5mjdvjh9++AGlpaV47733EBQUVOlrLl26hIULFyIvLw9GoxFarRYvv/wyDAYD\nZs6ciatXr0KpVKJdu3aYP38+Zs+ejT179sDf3x/r169HZGQkVqxYgYKCAnzwwQdo2rQpLl++DDc3\nN4wfPx7x8fG4fPky+vfvj1mzZsFkMiE2Nhbnz5+HwWCAEAIxMTHw9fXFiBEjkJ+fj/79+2PRokVI\nTExEfHw8lEolGjVqhLlz56Jly5aYMWMG8vLykJmZiV69eqF3795YvHgxTCYTAGDChAkICwuz15+d\n6orHmsSaqIb44osvREBAgLh48aIQQoi4uDgRGRlZ6e/fuXNHDBw4UKSnpwshhLh9+7YYMGCAOHfu\nnPjss8/EmDFjhBBClJaWitmzZ4srV64IIYTw9/cXt27dEkII0bt3b/HNN9+YP/vChQtCCCGioqLE\n8OHDRXFxsbh165Zo166d+PXXX8XXX38tJk2aJIxGoxBCiHXr1okJEyYIIYTYvXu3GD9+vBBCiFOn\nTol+/fqZP2f37t1iwIABwmQyiejoaDF69GjzfowaNUokJycLIYT47rvvxLx58x7/j0myw6EhqjN8\nfX0REBAAQJqE7rPPPqv0d69cuYKff/4Zs2bNMq8rKirCxYsXERISguXLl0Or1aJbt24YPXp0lXd/\natasGdq2bQtAukeEp6cnnJ2d0aBBA7i7u+P333/Hs88+C29vb+zYsQOZmZk4c+YM3N3dK7zXiRMn\nMHDgQPPkacOGDcPChQuRlZUFABa9nAEDBmD+/Pk4evQounXrhn/84x/V/GsRlePBYqozXF1dzT8r\nFAqIPxj1NBqN8PT0xOeff25+7Ny5E+Hh4WjevDkOHTqE8ePHQ6/X4/XXX8fRo0f/8LOdnZ0tlh80\nJfCxY8cwYcIEAEDfvn0xYsSIB75X2TDPvYQQ5vnm69WrZ14fERGBvXv3onv37jh58iQGDx5sMT01\nUXUwCEiWWrZsCVdXV3z++ecApPsbDBo0COnp6UhISMDMmTMRHByMadOmITg4GBcvXgTweDcASUlJ\nQe/evfHaa6+hffv2OHz4MIxGY4X3DQkJwb///W/z2Um7d+9G/fr1H9griYiIwHfffYdhw4ZhwYIF\nuH37Nm7evPlI9ZF8cWiIZMnZ2Rlr1qzBwoULsXHjRpSWlmLKlCkICgpCQEAAvvzySwwcOBBubm5o\n2rQptFotAOk2mVqt1uL2oNUVERGBqVOn4sUXX0RpaSm6d++O//znPzCZTAgMDMRHH32Et956C6tX\nr8bf/vY3jB492jy//rp16yxuvlPmnXfeQWxsLD788EMoFAq89dZbaNas2WP/fUheeNYQEZHMsUdA\nddbGjRuxb9++B26LiorC4MGD7VwRUc3EHgERkczxYDERkcwxCIiIZI5BQEQkcwwCIiKZYxAQEcnc\n/we8F7tK7XycmAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1953e694b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## now lets look at each hyperparamter in deails\n",
    "\n",
    "n_estimators = [1, 2, 4 ,8, 16, 32, 64, 100, 200, 500 ]\n",
    "train_results= []\n",
    "test_results = []\n",
    "\n",
    "for estimator in n_estimators:\n",
    "    rf = RandomForestClassifier(n_estimators = estimator, n_jobs =-1)\n",
    "    rf.fit(x_train, y_train)\n",
    "    train_pred = rf.predict(x_train)\n",
    "    \n",
    "    false_postive_rate, true_positive_rate, thresolds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(x = false_postive_rate, y = true_positive_rate)\n",
    "    train_results.append(roc_auc)\n",
    "    \n",
    "    y_pred = rf.predict(x_test)\n",
    "    false_positive_rate, true_positive_rate, thresolds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(x = false_postive_rate, y = true_positive_rate)\n",
    "    test_results.append(roc_auc)\n",
    "\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(n_estimators, train_results, 'b', label= 'Train AUC')\n",
    "line2,= plt.plot(n_estimators, test_results, 'r', label= 'Test AUC')\n",
    "\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.show()\n",
    "\n",
    "## N_estimators represent the number of trees in a model. Ususally higher the number of trees, better learns the model. \n",
    "## but a high number of trees also increase the computation time and slo down the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "C:\\Users\\HP\\Anaconda2\\envs\\py36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:248: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEECAYAAAAlEzNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1953d065da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## MAX_DEPTH\n",
    "\n",
    "max_depths = np.linspace(1, 32, 32, endpoint = True)\n",
    "\n",
    "train_results =[]\n",
    "test_results = []\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    rf = RandomForestClassifier(max_depth = max_depth, n_jobs=-1)\n",
    "    rf.fit(x_train, y_train)\n",
    "    train_pred = rf.predict(x_train)\n",
    "    \n",
    "    false_positive_rate, true_positive_rate, thresolds = roc_curve(y_train, train_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    train_results.append(roc_auc)\n",
    "    \n",
    "    test_pred = rf.predict(x_test)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, test_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    test_results.append(roc_auc)\n",
    "    \n",
    "from matplotlib.legend_handler import HandlerLine2D\n",
    "line1, = plt.plot(max_depths, train_results, 'b', label= 'Train AUC')\n",
    "line2, = plt.plot(max_depths, test_results, 'r', label= 'Test AUC')\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.ylabel('AUC score')\n",
    "plt.xlabel('Tree depth')\n",
    "plt.show()\n",
    "\n",
    "## max_depth represents the depth of each tree in the forest. The deeper the tree, the more splits it has and it captures more information about the data. \n",
    "## We see that our model overfits for large depth values. \n",
    "## the trees perfectly predicts all of the train data, however, it fails to generalize the findings for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## min_samples_split\n",
    "https://medium.com/all-things-ai/in-depth-parameter-tuning-for-random-forest-d67bb7e920d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
